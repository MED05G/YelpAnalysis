{
  "metadata": {
    "name": "Yelp_Data_Analysis",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n%pyspark\nfrom pyspark.sql import SparkSession\n\nspark \u003d SparkSession.builder \\\n    .appName(\"Yelp Data Analysis\") \\\n    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n    .config(\"spark.sql.warehouse.dir\", \"/user/hive/warehouse\") \\\n    .config(\"spark.hadoop.hive.metastore.uris\", \"thrift://node-master:9083\") \\\n    .enableHiveSupport() \\\n    .getOrCreate()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nspark \u003d SparkSession.builder \\\n    .appName(\"Yelp Data Analysis\") \\\n    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n    .config(\"spark.sql.warehouse.dir\", \"/user/hive/warehouse\") \\\n    .config(\"spark.hadoop.hive.metastore.uris\", \"thrift://node-master:9083\") \\\n    .enableHiveSupport() \\\n    .getOrCreate()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# Load cleaned Hive tables\nbusiness \u003d spark.sql(\"SELECT * FROM business\")\ncheckin \u003d spark.sql(\"SELECT * FROM checkin\")\nreview \u003d spark.sql(\"SELECT * FROM review\")\nuser \u003d spark.sql(\"SELECT * FROM users\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "1: Business Analysis"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Q1: Top 20 most common merchants in the U.S.\nbusiness.groupBy(\u0027name\u0027).count().orderBy(desc(\u0027count\u0027)).show(20, truncate\u003dFalse)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# 2. Top 10 Cities with the Most Merchants in the U.S.\nbusiness.groupBy(\u0027city\u0027).count().orderBy(desc(\u0027count\u0027)).show(10, truncate\u003dFalse)"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# 3. Top 5 States with the Most Merchants in the U.S.\nbusiness.groupBy(\u0027state\u0027).count().orderBy(desc(\u0027count\u0027)).show(5, truncate\u003dFalse)"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# 4. Top 20 Most Common Merchants with Their Average Ratings\nbusiness.groupBy(\u0027name\u0027).agg(avg(\u0027stars\u0027).alias(\u0027average_rating\u0027)).orderBy(desc(\u0027average_rating\u0027)).show(20, truncate\u003dFalse)"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# 5. Top 10 Cities with the Highest Ratings\nbusiness.groupBy(\u0027city\u0027).agg(avg(\u0027stars\u0027).alias(\u0027average_rating\u0027)).orderBy(desc(\u0027average_rating\u0027)).show(10, truncate\u003dFalse)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# 6. Count of Different Categories\nbusiness.select(explode(split(col(\u0027categories\u0027), \u0027, \u0027)).alias(\u0027category\u0027)).groupBy(\u0027category\u0027).count().orderBy(desc(\u0027count\u0027)).show(10, truncate\u003dFalse)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# 7. Top 10 Most Frequent Categories and Their Count\ncategory_counts \u003d business.select(explode(split(col(\u0027categories\u0027), \u0027, \u0027)).alias(\u0027category\u0027)) \\\n    .groupBy(\u0027category\u0027).count().orderBy(desc(\u0027count\u0027)).limit(10).toPandas()\n\n# Plot for Query 7: Top 10 Most Frequent Categories\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize\u003d(10, 6))\nsns.barplot(data\u003dcategory_counts, x\u003d\u0027category\u0027, y\u003d\u0027count\u0027, palette\u003d\u0027viridis\u0027)\nplt.xticks(rotation\u003d45, ha\u003d\u0027right\u0027)\nplt.title(\u0027Top 10 Most Frequent Categories\u0027)\nplt.xlabel(\u0027Category\u0027)\nplt.ylabel(\u0027Count\u0027)\nplt.tight_layout()\nplt.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ntop_five_star_merchants \u003d review.filter(col(\u0027rev_stars\u0027) \u003d\u003d 5) \\\n    .groupBy(\u0027rev_business_id\u0027).count() \\\n    .orderBy(desc(\u0027count\u0027)).join(business, review.rev_business_id \u003d\u003d business.business_id, \u0027inner\u0027) \\\n    .select(\u0027name\u0027, \u0027count\u0027).limit(20).toPandas()\n\n# Plot for Query 8: Top 20 Merchants with the Most Five-Star Reviews\nplt.figure(figsize\u003d(10, 6))\nsns.barplot(data\u003dtop_five_star_merchants, y\u003d\u0027name\u0027, x\u003d\u0027count\u0027, palette\u003d\u0027Blues_d\u0027)\nplt.title(\u0027Top 20 Merchants with the Most Five-Star Reviews\u0027)\nplt.xlabel(\u0027Number of Reviews\u0027)\nplt.ylabel(\u0027Merchant\u0027)\nplt.tight_layout()\nplt.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# 9. Count the Number of Restaurant Types (Chinese, American, Mexican)\nrestaurant_types \u003d [\u0027Chinese\u0027, \u0027American\u0027, \u0027Mexican\u0027]\n\nfor cuisine in restaurant_types:\n    print(f\"{cuisine} Restaurants Count:\")\n    business.filter(col(\u0027categories\u0027).contains(cuisine)).count()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfor cuisine in restaurant_types:\n    print(f\"Reviews for {cuisine}:\")\n    review.join(business.filter(col(\u0027categories\u0027).contains(cuisine)), review.rev_business_id \u003d\u003d business.business_id, \u0027inner\u0027).count()"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfor cuisine in restaurant_types:\n    cuisine_reviews \u003d review.join(business.filter(col(\u0027categories\u0027).contains(cuisine)),\n                                  review.rev_business_id \u003d\u003d business.business_id, \u0027inner\u0027) \\\n        .groupBy(\u0027rev_stars\u0027).count().orderBy(\u0027rev_stars\u0027).toPandas()\n\n    # Plot for Query 11: Rating Distribution for Different Restaurant Types\n    plt.figure(figsize\u003d(8, 5))\n    sns.barplot(data\u003dcuisine_reviews, x\u003d\u0027rev_stars\u0027, y\u003d\u0027count\u0027, palette\u003d\u0027viridis\u0027)\n    plt.title(f\u0027Rating Distribution for {cuisine} Restaurants\u0027)\n    plt.xlabel(\u0027Rating Stars\u0027)\n    plt.ylabel(\u0027Count\u0027)\n    plt.tight_layout()\n    plt.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "2: User Analysis:"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark \n# 1. Number of users joining each year\nuser.withColumn(\"join_year\", year(to_timestamp(col(\"user_yelping_since\"), \"yyyy-MM-dd HH:mm:ss\"))) \\\n    .groupBy(\"join_year\").count().orderBy(\"join_year\").show(100, truncate\u003dFalse)"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# 2. Top reviewers by review_count\nuser.select(\"user_id\", \"user_name\", \"user_review_count\") \\\n    .orderBy(desc(\"user_review_count\")).show(20, truncate\u003dFalse)"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql.functions import col, year, to_timestamp, sum, count, round\n\n# 3.Identify the most popular users based on fans\nusers_with_fans \u003d user.withColumn(\"join_year\", year(to_timestamp(col(\"user_yelping_since\"), \"yyyy-MM-dd HH:mm:ss\"))) \\\n    .withColumn(\"fan_count\", col(\"user_fans\")) \\\n    .groupBy(\"join_year\") \\\n    .agg(\n        count(\"*\").alias(\"total_users\"),\n        sum(\"fan_count\").alias(\"total_fans\")  \n    ) \\\n    .withColumn(\"avg_fans\", round(col(\"total_fans\") / col(\"total_users\"), 2)) \\\n    .orderBy(\"join_year\")\n\n\nusers_with_fans.show(100, truncate\u003dFalse)\n\n\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql.functions import col, year, to_timestamp, when, count, sum, round\n\n#4.Calculate the ratio of elite users to regular users each year.\nusers_with_elite \u003d user.withColumn(\n    \"join_year\", year(to_timestamp(col(\"user_yelping_since\"), \"yyyy-MM-dd HH:mm:ss\"))\n).withColumn(\n    \"is_elite\", when(col(\"user_elite\").isNotNull() \u0026 (col(\"user_elite\") !\u003d \"\"), 1).otherwise(0)\n)\n\n\nelite_stats \u003d users_with_elite.groupBy(\"join_year\").agg(\n    count(\"*\").alias(\"total_users\"),\n    sum(\"is_elite\").alias(\"elite_users\")\n).withColumn(\n    \"regular_users\", col(\"total_users\") - col(\"elite_users\")\n).withColumn(\n    \"elite_ratio\", round(col(\"elite_users\") / col(\"regular_users\"), 4)  \n).orderBy(\"join_year\")\n\n\nelite_stats.show(100, truncate\u003dFalse)\n\n\n\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# 5. Display the Proportion of Total Users and Silent Users (No Reviews) Each Year\nuser.withColumn(\"join_year\", year(to_timestamp(col(\"user_yelping_since\"), \"yyyy-MM-dd HH:mm:ss\"))) \\\n    .withColumn(\"is_silent\", when(col(\"user_review_count\") \u003d\u003d 0, 1).otherwise(0)) \\\n    .groupBy(\"join_year\").agg(count(\"*\").alias(\"total_users\"), sum(\"is_silent\").alias(\"silent_users\")) \\\n    .withColumn(\"silent_ratio\", round(col(\"silent_users\") / col(\"total_users\"), 4)) \\\n    .orderBy(\"join_year\").show(100, truncate\u003dFalse)"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# 6. Compute the Yearly Statistics of New Users, Number of Reviews, Elite Users, and Fans\nfrom pyspark.sql.functions import col, year, to_timestamp, when, length, count, sum, round, explode, split, trim\n\nbusiness \u003d spark.sql(\"SELECT * FROM business\")\ncheckin \u003d spark.sql(\"SELECT * FROM checkin\")\nreview \u003d spark.sql(\"SELECT * FROM review\")\nuser \u003d spark.sql(\"SELECT * FROM users\")\n\nuser_stats \u003d user.withColumn(\"join_year\", year(to_timestamp(trim(col(\"user_yelping_since\")), \"yyyy-MM-dd HH:mm:ss\"))) \\\n    .withColumn(\"is_elite\", when(length(col(\"user_elite\")) \u003e 0, 1).otherwise(0)) \\\n    .groupBy(\"join_year\") \\\n    .agg(\n        count(\"*\").alias(\"new_users\"),\n        sum(\"user_review_count\").alias(\"total_reviews\"),\n        sum(\"is_elite\").alias(\"elite_users\"),\n        sum(\"user_fans\").alias(\"total_fans\")\n    )\n\ncheckin_exploded \u003d checkin.withColumn(\"checkin_date\", explode(split(col(\"checkin_dates\"), \",\"))) \\\n    .withColumn(\"join_year\", year(to_timestamp(trim(col(\"checkin_date\")), \"yyyy-MM-dd HH:mm:ss\")))\n\ncheckin_stats \u003d checkin_exploded.groupBy(\"join_year\").agg(\n    count(\"*\").alias(\"total_checkins\")\n)\n\nfinal_stats \u003d user_stats \\\n    .join(checkin_stats, on\u003d\"join_year\", how\u003d\"left\") \\\n    .fillna(0, subset\u003d[\"total_checkins\"])\n\nfinal_stats.orderBy(\"join_year\").show(100, truncate\u003dFalse)\n\n\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " \n3: Review Analysis:"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql.functions import *\n\n# 1.Count number of reviews per year\nreview.withColumn(\"review_year\", year(to_date(col(\"rev_date\"), \"yyyy-MM-dd\"))) \\\n      .groupBy(\"review_year\") \\\n      .agg(count(\"*\").alias(\"total_reviews\")) \\\n      .orderBy(\"review_year\") \\\n      .show(100, False)"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# 2.Count useful, funny, and cool reviews\nreview.groupBy(\"rev_stars\").agg(\n    sum(\"rev_useful\").alias(\"total_useful\"),\n    sum(\"rev_funny\").alias(\"total_funny\"),\n    sum(\"rev_cool\").alias(\"total_cool\")\n).orderBy(\"rev_stars\").show(100, False)"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n#3. Rank users by the total number of reviews each year:\nreview.withColumn(\"review_year\", year(to_date(col(\"rev_date\"), \"yyyy-MM-dd\"))) \\\n      .groupBy(\"review_year\", \"rev_user_id\") \\\n      .agg(count(\"*\").alias(\"review_count\")) \\\n      .orderBy(desc(\"review_count\")) \\\n      .show(100, False)"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n#4.Extract the Top 20 most common words from all reviews.\n\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover\nfrom pyspark.sql.functions import col\nfrom pyspark.ml import Pipeline\nfrom pyspark.sql import functions as F\nfrom pyspark.ml.feature import CountVectorizer\n\n# Step 1: Tokenize the reviews into words\ntokenizer \u003d Tokenizer(inputCol\u003d\"rev_text\", outputCol\u003d\"words\")\n\n# Step 2: Remove stopwords from the reviews\nstopwords_remover \u003d StopWordsRemover(inputCol\u003d\"words\", outputCol\u003d\"filtered_words\")\n\n# Step 3: Create a pipeline for preprocessing\npipeline \u003d Pipeline(stages\u003d[tokenizer, stopwords_remover])\n\n# Step 4: Fit the pipeline and transform the reviews data\nprocessed_reviews \u003d pipeline.fit(review).transform(review)\n\n# Step 5: Flatten the array of words into individual rows\nwords_df \u003d processed_reviews.select(F.explode(\"filtered_words\").alias(\"word\"))\n\n# Step 6: Count the frequency of each word\nword_counts \u003d words_df.groupBy(\"word\").count()\n\n# Step 7: Order the words by frequency and get the top 20\ntop_20_words \u003d word_counts.orderBy(col(\"count\").desc()).limit(20)\n\n# Step 8: Show the results\ntop_20_words.show(truncate\u003dFalse)\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nfrom pyspark.sql import functions as F\nfrom pyspark.ml.feature import StopWordsRemover, Tokenizer\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.sql import Row\nimport re\n\n# Step 1: Load the reviews DataFrame (make sure you load your reviews data first)\n# reviews \u003d spark.read.parquet(\"path_to_reviews_data\")  # Example path\n\n# Step 2: Add extra stopwords (optional)\nadditional_stopwords \u003d [\"get\", \"really\", \"like\", \"also\", \"one\", \"will\", \"would\", \"can\", \"could\"]\n\n# Function to add custom stopwords\ndef add_stopwords():\n    stopwords \u003d StopWordsRemover.loadDefaultStopWords(\"english\")\n    stopwords.extend(additional_stopwords)\n    return stopwords\n\n# Step 3: Tokenize the reviews into words\ntokenizer \u003d Tokenizer(inputCol\u003d\"rev_text\", outputCol\u003d\"words\")\n\n# Step 4: Remove stopwords\nremover \u003d StopWordsRemover(inputCol\u003d\"words\", outputCol\u003d\"filtered_words\", stopWords\u003dadd_stopwords())\n\n# Step 5: Create a pipeline for tokenization and stopword removal\npipeline \u003d Pipeline(stages\u003d[tokenizer, remover])\n\n# Step 6: Apply the pipeline on the reviews data (Ensure your reviews DataFrame is loaded)\nprocessed_reviews \u003d pipeline.fit(reviews).transform(reviews)\n\n# Step 7: Filter for positive reviews (rating \u003e 3)\npositive_reviews \u003d processed_reviews.filter(F.col(\"rev_stars\") \u003e 3)\n\n# Step 8: Explode the filtered words to count occurrences\nword_counts \u003d positive_reviews.withColumn(\"word\", F.explode(F.col(\"filtered_words\"))) \\\n    .groupBy(\"word\") \\\n    .count() \\\n    .orderBy(\"count\", ascending\u003dFalse)\n\n# Step 9: Show the top 10 words\nword_counts.show(10, truncate\u003dFalse)\n\n\n   \n\n\n\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n#5.Extract the Top 10 words from positive reviews (rating \u003e 3).\nfrom pyspark.sql import functions as F\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover, NGram\nfrom pyspark.ml import Pipeline\n\n# Step 1: Tokenize the reviews into words\ntokenizer \u003d Tokenizer(inputCol\u003d\"rev_text\", outputCol\u003d\"words\")\n\n# Step 2: Remove stopwords (default stopwords plus any additional ones)\nadditional_stopwords \u003d [\"get\", \"really\", \"like\", \"also\", \"one\", \"will\", \"would\", \"can\", \"could\"]\n\ndef add_stopwords():\n    stopwords \u003d StopWordsRemover.loadDefaultStopWords(\"english\")\n    stopwords.extend(additional_stopwords)\n    return stopwords\n\n# Step 3: Remove stopwords\nremover \u003d StopWordsRemover(inputCol\u003d\"words\", outputCol\u003d\"filtered_words\", stopWords\u003dadd_stopwords())\n\n# Step 4: Generate n-grams (phrases). In this case, let\u0027s generate bigrams (2-word phrases)\nngram \u003d NGram(n\u003d2, inputCol\u003d\"filtered_words\", outputCol\u003d\"bigrams\")\n\n# Step 5: Create a pipeline with tokenization, stopword removal, and n-gram generation\npipeline \u003d Pipeline(stages\u003d[tokenizer, remover, ngram])\n\n# Step 6: Apply the pipeline on the reviews data (ensure your reviews DataFrame is loaded)\nprocessed_reviews \u003d pipeline.fit(review).transform(review)\n\n# Step 7: Filter for positive reviews (rating \u003e 3)\npositive_reviews \u003d processed_reviews.filter(F.col(\"rev_stars\") \u003e 3)\n\n# Step 8: Explode the n-grams to count occurrences\nword_counts \u003d positive_reviews.withColumn(\"bigram\", F.explode(F.col(\"bigrams\"))) \\\n    .groupBy(\"bigram\") \\\n    .count() \\\n    .orderBy(\"count\", ascending\u003dFalse)\n\n# Step 9: Show the top 10 bigrams (phrases)\nword_counts.show(10, truncate\u003dFalse)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql import functions as F\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover, NGram\nfrom pyspark.ml import Pipeline\n\n# Step 1: Tokenize the reviews into words\ntokenizer \u003d Tokenizer(inputCol\u003d\"rev_text\", outputCol\u003d\"words\")\n\n# Step 2: Remove stopwords (default stopwords plus any additional ones)\nadditional_stopwords \u003d [\"get\", \"really\", \"like\", \"also\", \"one\", \"will\", \"would\", \"can\", \"could\"]\n\ndef add_stopwords():\n    stopwords \u003d StopWordsRemover.loadDefaultStopWords(\"english\")\n    stopwords.extend(additional_stopwords)\n    return stopwords\n\n# Step 3: Remove stopwords\nremover \u003d StopWordsRemover(inputCol\u003d\"words\", outputCol\u003d\"filtered_words\", stopWords\u003dadd_stopwords())\n\n# Step 4: Generate n-grams (phrases). In this case, let\u0027s generate bigrams (2-word phrases)\nngram \u003d NGram(n\u003d2, inputCol\u003d\"filtered_words\", outputCol\u003d\"bigrams\")\n\n# Step 5: Create a pipeline with tokenization, stopword removal, and n-gram generation\npipeline \u003d Pipeline(stages\u003d[tokenizer, remover, ngram])\n\n# Step 6: Apply the pipeline on the reviews data (ensure your reviews DataFrame is loaded)\nprocessed_reviews \u003d pipeline.fit(review).transform(review)\n\n# Step 7: Filter for negative reviews (rating ≤ 3)\nnegative_reviews \u003d processed_reviews.filter(F.col(\"rev_stars\") \u003c\u003d 3)\n\n# Step 8: Explode the n-grams to count occurrences\nword_counts \u003d negative_reviews.withColumn(\"bigram\", F.explode(F.col(\"bigrams\"))) \\\n    .groupBy(\"bigram\") \\\n    .count() \\\n    .orderBy(\"count\", ascending\u003dFalse)\n\n# Step 9: Show the top 10 bigrams (phrases)\nword_counts.show(10, truncate\u003dFalse)\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "4: Rating Analysis:"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql.functions import col, count\n#1.Analyze the distribution of ratings (1-5 stars).\nreview \u003d spark.sql(\"SELECT * FROM review\")\n\nrating_distribution \u003d review.groupBy(\"rev_stars\").agg(count(\"*\").alias(\"num_reviews\")).orderBy(\"rev_stars\")\n\nrating_distribution.show(5, truncate\u003dFalse)\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql.functions import date_format\n#2.Analyze the weekly rating frequency (Monday to Sunday).\nweekly_rating_freq \u003d review.withColumn(\"day_of_week\", date_format(col(\"rev_date\"), \"EEEE\")) \\\n    .groupBy(\"day_of_week\").agg(count(\"*\").alias(\"num_reviews\")) \\\n    .orderBy(\"num_reviews\", ascending\u003dFalse)\n\nweekly_rating_freq.show(7, truncate\u003dFalse)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n#3.Identify the top businesses with the most five-star ratings.\n\nbusiness \u003d spark.sql(\"SELECT * FROM business\")\n\ntop_five_star_businesses \u003d review.filter(col(\"rev_stars\") \u003d\u003d 5) \\\n    .groupBy(\"rev_business_id\") \\\n    .agg(count(\"*\").alias(\"five_star_count\")) \\\n    .join(business, business.business_id \u003d\u003d review.rev_business_id, \"inner\") \\\n    .select(\"name\", \"five_star_count\") \\\n    .orderBy(desc(\"five_star_count\")) \\\n    .limit(10)\n\ntop_five_star_businesses.show(10, truncate\u003dFalse)\n\n\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "5: CHECK IN ANALYSIS"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql.functions import split, explode, to_timestamp, year, hour, count, trim, col\n# 1. Count the number of check-ins per year\n\ncheckin_exploded \u003d checkin.withColumn(\"checkin_date\", explode(split(col(\"checkin_dates\"), \",\")))\n\n\ncheckin_exploded_with_year \u003d checkin_exploded.withColumn(\"checkin_date_ts\", to_timestamp(trim(col(\"checkin_date\")), \"yyyy-MM-dd HH:mm:ss\")) \\\n    .withColumn(\"year\", year(col(\"checkin_date_ts\")))\n\n\ncheckin_per_year \u003d checkin_exploded_with_year.groupBy(\"year\").agg(count(\"*\").alias(\"checkin_count\")).orderBy(\"year\")\n\n\ncheckin_per_year_pd \u003d checkin_per_year.toPandas()\n\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize\u003d(10,6))\nplt.plot(checkin_per_year_pd[\u0027year\u0027], checkin_per_year_pd[\u0027checkin_count\u0027], marker\u003d\u0027o\u0027, linestyle\u003d\u0027-\u0027, color\u003d\u0027b\u0027)\nplt.title(\u0027Check-ins per Year\u0027)\nplt.xlabel(\u0027Year\u0027)\nplt.ylabel(\u0027Number of Check-ins\u0027)\nplt.grid(True)\nplt.xticks(rotation\u003d45)\nplt.tight_layout()\n\n\nplt.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# 2. Count the number of check-ins per hour within a 24-hour period\n\ncheckin_per_hour \u003d checkin_exploded.withColumn(\"checkin_date_ts\", to_timestamp(trim(col(\"checkin_date\")), \"yyyy-MM-dd HH:mm:ss\")) \\\n    .withColumn(\"hour\", hour(col(\"checkin_date_ts\"))) \\\n    .groupBy(\"hour\").agg(count(\"*\").alias(\"checkin_count\")) \\\n    .orderBy(\"hour\")\n\n\ncheckin_per_hour_pd \u003d checkin_per_hour.toPandas()\n\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize\u003d(10,6))\nplt.plot(checkin_per_hour_pd[\u0027hour\u0027], checkin_per_hour_pd[\u0027checkin_count\u0027], marker\u003d\u0027o\u0027, linestyle\u003d\u0027-\u0027, color\u003d\u0027g\u0027)\nplt.title(\u0027Check-ins per Hour\u0027)\nplt.xlabel(\u0027Hour of Day\u0027)\nplt.ylabel(\u0027Number of Check-ins\u0027)\nplt.grid(True)\nplt.xticks(range(0, 24))  \nplt.tight_layout()\n\n\nplt.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# 3. Identify the most popular city for check-ins\n\n\ncheckin_exploded.join(business, checkin_exploded.business_id \u003d\u003d business.business_id, \"inner\") \\\n    .groupBy(\"city\").agg(count(\"*\").alias(\"total_checkins\")) \\\n    .orderBy(col(\"total_checkins\").desc()).show(10, False)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# 4.rank all business based on check in counts\n\n\n\ncheckin_exploded \u003d checkin.withColumn(\"checkin_date\", explode(split(col(\"checkin_dates\"), \",\")))\n\n\ncheckin_count_per_business \u003d checkin_exploded.withColumn(\"checkin_date_ts\", to_timestamp(trim(col(\"checkin_date\")), \"yyyy-MM-dd HH:mm:ss\")) \\\n    .groupBy(\"business_id\").agg(count(\"*\").alias(\"checkin_count\"))\n\n\ncheckin_count_with_names \u003d checkin_count_per_business.join(business, \"business_id\", \"inner\")\n\n\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import rank\n\nwindowSpec \u003d Window.orderBy(col(\"checkin_count\").desc())\n\n\nranked_businesses \u003d checkin_count_with_names.withColumn(\"rank\", rank().over(windowSpec))\n\n\nranked_businesses.select(\"name\", \"checkin_count\", \"rank\").orderBy(\"rank\").show(10, False)\n\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "6.COMPREHENSIVE ANALYSIS"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n#1.Identify the top 5 merchants in each city based on rating frequency, average rating, and check-in frequency.\nfrom pyspark.sql.functions import col, count, avg, split, explode, row_number\nfrom pyspark.sql.window import Window\n\n# --- Review count and average rating per business ---\nreview_stats \u003d review.groupBy(\"rev_business_id\").agg(\n    count(\"*\").alias(\"review_count\"),\n    avg(\"rev_stars\").alias(\"average_rating\")\n)\n\n# --- Check-in count per business ---\ncheckin_exploded \u003d checkin.withColumn(\"checkin_date\", explode(split(col(\"checkin_dates\"), \",\")))\ncheckin_stats \u003d checkin_exploded.groupBy(\"business_id\").agg(count(\"*\").alias(\"checkin_count\"))\n\n# --- Join all stats with business info ---\nbiz_info \u003d business.select(\"business_id\", \"name\", \"city\")\nstats \u003d biz_info \\\n    .join(review_stats, biz_info.business_id \u003d\u003d review_stats.rev_business_id, \"left\") \\\n    .join(checkin_stats, biz_info.business_id \u003d\u003d checkin_stats.business_id, \"left\") \\\n    .fillna(0, subset\u003d[\"review_count\", \"average_rating\", \"checkin_count\"])\n\n# --- Rank top 5 merchants in each city ---\nwindow \u003d Window.partitionBy(\"city\").orderBy(\n    col(\"review_count\").desc(),\n    col(\"average_rating\").desc(),\n    col(\"checkin_count\").desc()\n)\ntop5 \u003d stats.withColumn(\"rank\", row_number().over(window)).filter(col(\"rank\") \u003c\u003d 5)\n\n# --- Show final result ---\ntop5.select(\"city\", \"name\", \"review_count\", \"average_rating\", \"checkin_count\", \"rank\") \\\n    .orderBy(\"city\", \"rank\").show(100, False)\n\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}