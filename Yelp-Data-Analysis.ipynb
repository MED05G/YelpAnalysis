{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Business Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark \n",
    "# 1. Number of users joining each year\n",
    "user.withColumn(\"join_year\", year(to_timestamp(col(\"user_yelping_since\"), \"yyyy-MM-dd HH:mm:ss\"))) \\\n",
    "    .groupBy(\"join_year\").count().orderBy(\"join_year\").show(100, truncate=False)\n",
    "\n",
    "#by Mohamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "# 2. Top reviewers by review_count\n",
    "user.select(\"user_id\", \"user_name\", \"user_review_count\") \\\n",
    "    .orderBy(desc(\"user_review_count\")).show(20, truncate=False)\n",
    "\n",
    "#by Mohamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "from pyspark.sql.functions import col, year, to_timestamp, sum, count, round\n",
    "\n",
    "# 3.Identify the most popular users based on fans\n",
    "users_with_fans = user.withColumn(\"join_year\", year(to_timestamp(col(\"user_yelping_since\"), \"yyyy-MM-dd HH:mm:ss\"))) \\\n",
    "    .withColumn(\"fan_count\", col(\"user_fans\")) \\\n",
    "    .groupBy(\"join_year\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_users\"),\n",
    "        sum(\"fan_count\").alias(\"total_fans\")  \n",
    "    ) \\\n",
    "    .withColumn(\"avg_fans\", round(col(\"total_fans\") / col(\"total_users\"), 2)) \\\n",
    "    .orderBy(\"join_year\")\n",
    "\n",
    "\n",
    "users_with_fans.show(100, truncate=False)\n",
    "\n",
    "\n",
    "\n",
    "#by Mohamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "from pyspark.sql.functions import col, year, to_timestamp, when, count, sum, round\n",
    "\n",
    "#4.Calculate the ratio of elite users to regular users each year.\n",
    "users_with_elite = user.withColumn(\n",
    "    \"join_year\", year(to_timestamp(col(\"user_yelping_since\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    ").withColumn(\n",
    "    \"is_elite\", when(col(\"user_elite\").isNotNull() & (col(\"user_elite\") != \"\"), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "\n",
    "elite_stats = users_with_elite.groupBy(\"join_year\").agg(\n",
    "    count(\"*\").alias(\"total_users\"),\n",
    "    sum(\"is_elite\").alias(\"elite_users\")\n",
    ").withColumn(\n",
    "    \"regular_users\", col(\"total_users\") - col(\"elite_users\")\n",
    ").withColumn(\n",
    "    \"elite_ratio\", round(col(\"elite_users\") / col(\"regular_users\"), 4)  \n",
    ").orderBy(\"join_year\")\n",
    "\n",
    "\n",
    "elite_stats.show(100, truncate=False)\n",
    "\n",
    "\n",
    "\n",
    "#by Mohamed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "# 5. Display the Proportion of Total Users and Silent Users (No Reviews) Each Year\n",
    "user.withColumn(\"join_year\", year(to_timestamp(col(\"user_yelping_since\"), \"yyyy-MM-dd HH:mm:ss\"))) \\\n",
    "    .withColumn(\"is_silent\", when(col(\"user_review_count\") == 0, 1).otherwise(0)) \\\n",
    "    .groupBy(\"join_year\").agg(count(\"*\").alias(\"total_users\"), sum(\"is_silent\").alias(\"silent_users\")) \\\n",
    "    .withColumn(\"silent_ratio\", round(col(\"silent_users\") / col(\"total_users\"), 4)) \\\n",
    "    .orderBy(\"join_year\").show(100, truncate=False)\n",
    "\n",
    "#by Mohamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "# 6. Compute the Yearly Statistics of New Users, Number of Reviews, Elite Users, and Fans\n",
    "from pyspark.sql.functions import col, year, to_timestamp, when, length, count, sum, round, explode, split, trim\n",
    "\n",
    "business = spark.sql(\"SELECT * FROM business\")\n",
    "checkin = spark.sql(\"SELECT * FROM checkin\")\n",
    "review = spark.sql(\"SELECT * FROM review\")\n",
    "user = spark.sql(\"SELECT * FROM users\")\n",
    "\n",
    "user_stats = user.withColumn(\"join_year\", year(to_timestamp(trim(col(\"user_yelping_since\")), \"yyyy-MM-dd HH:mm:ss\"))) \\\n",
    "    .withColumn(\"is_elite\", when(length(col(\"user_elite\")) > 0, 1).otherwise(0)) \\\n",
    "    .groupBy(\"join_year\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"new_users\"),\n",
    "        sum(\"user_review_count\").alias(\"total_reviews\"),\n",
    "        sum(\"is_elite\").alias(\"elite_users\"),\n",
    "        sum(\"user_fans\").alias(\"total_fans\")\n",
    "    )\n",
    "\n",
    "checkin_exploded = checkin.withColumn(\"checkin_date\", explode(split(col(\"checkin_dates\"), \",\"))) \\\n",
    "    .withColumn(\"join_year\", year(to_timestamp(trim(col(\"checkin_date\")), \"yyyy-MM-dd HH:mm:ss\")))\n",
    "\n",
    "checkin_stats = checkin_exploded.groupBy(\"join_year\").agg(\n",
    "    count(\"*\").alias(\"total_checkins\")\n",
    ")\n",
    "\n",
    "final_stats = user_stats \\\n",
    "    .join(checkin_stats, on=\"join_year\", how=\"left\") \\\n",
    "    .fillna(0, subset=[\"total_checkins\"])\n",
    "\n",
    "final_stats.orderBy(\"join_year\").show(100, truncate=False)\n",
    "\n",
    "\n",
    "#by Mohamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%pyspark\n",
    "\n",
    "# 4. Extract Top 20 most common words from all reviews (stopwords removed)\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from pyspark.sql.functions import explode, col, lower\n",
    "\n",
    "# Tokenize review texts\n",
    "tokenized = Tokenizer(inputCol=\"rev_text\", outputCol=\"words\").transform(review)\n",
    " \n",
    "# Remove stopwords\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "filtered = remover.transform(tokenized)\n",
    "\n",
    "# Explode words, convert to lowercase, group, count, order, and show top 20\n",
    "filtered.select(explode(col(\"filtered_words\")).alias(\"word\")) \\\n",
    "    .select(lower(col(\"word\")).alias(\"word\")) \\\n",
    "    .groupBy(\"word\").count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show(20, truncate=False)\n",
    "\n",
    "\n",
    "#by RIDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "# 5. Extract Top 10 words from positive reviews (rating > 3), stopwords removed\n",
    "positive_reviews = review.filter(col(\"rev_stars\") > 3)\n",
    "\n",
    "# Tokenize and remove stopwords\n",
    "tokenized_pos = Tokenizer(inputCol=\"rev_text\", outputCol=\"words\").transform(positive_reviews)\n",
    "filtered_pos = remover.transform(tokenized_pos)\n",
    "\n",
    "# Explode, lowercase, count, and show top 10\n",
    "filtered_pos.select(explode(col(\"filtered_words\")).alias(\"word\")) \\\n",
    "    .select(lower(col(\"word\")).alias(\"word\")) \\\n",
    "    .groupBy(\"word\").count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show(10, truncate=False)\n",
    "\n",
    "\n",
    "#by RIDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "# 6. Extract Top 10 words from negative reviews (rating â‰¤ 3), stopwords removed\n",
    "negative_reviews = review.filter(col(\"rev_stars\") <= 3)\n",
    "\n",
    "# Tokenize and remove stopwords\n",
    "tokenized_neg = Tokenizer(inputCol=\"rev_text\", outputCol=\"words\").transform(negative_reviews)\n",
    "filtered_neg = remover.transform(tokenized_neg)\n",
    "\n",
    "# Explode, lowercase, count, and show top 10\n",
    "filtered_neg.select(explode(col(\"filtered_words\")).alias(\"word\")) \\\n",
    "    .select(lower(col(\"word\")).alias(\"word\")) \\\n",
    "    .groupBy(\"word\").count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show(10, truncate=False)\n",
    "\n",
    "\n",
    "#by RIDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rating Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark \n",
    "\n",
    "from pyspark.sql.functions import split, explode, to_timestamp, year, hour, count, trim, col\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 1. Count the number of check-ins per year\n",
    "# ---------------------------------------------\n",
    "# Step 1: Split check-in dates and explode into individual timestamps\n",
    "checkin_exploded = checkin.withColumn(\"checkin_date\", explode(split(col(\"checkin_dates\"), \",\")))\n",
    "\n",
    "# Step 2: Extract year and count check-ins per year\n",
    "checkin_exploded.withColumn(\"checkin_date_ts\", to_timestamp(trim(col(\"checkin_date\")), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "    .withColumn(\"year\", year(col(\"checkin_date_ts\"))) \\\n",
    "    .groupBy(\"year\").agg(count(\"*\").alias(\"checkin_count\")) \\\n",
    "    .orderBy(\"year\").show(10, False)\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 2. Count the number of check-ins per hour within a 24-hour period\n",
    "# ---------------------------------------------\n",
    "# Step 1 & 2 reused: already exploded and timestamped\n",
    "# Step 3: Extract hour and count\n",
    "checkin_exploded.withColumn(\"checkin_date_ts\", to_timestamp(trim(col(\"checkin_date\")), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "    .withColumn(\"hour\", hour(col(\"checkin_date_ts\"))) \\\n",
    "    .groupBy(\"hour\").agg(count(\"*\").alias(\"checkin_count\")) \\\n",
    "    .orderBy(\"hour\").show(24, False)\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 3. Identify the most popular city for check-ins\n",
    "# ---------------------------------------------\n",
    "# Join check-in data with business to get city names, then count check-ins per city\n",
    "checkin_exploded.join(business, checkin_exploded.business_id == business.business_id, \"inner\") \\\n",
    "    .groupBy(\"city\").agg(count(\"*\").alias(\"total_checkins\")) \\\n",
    "    .orderBy(col(\"total_checkins\").desc()).show(10, False)\n",
    "\n",
    "\n",
    "\n",
    "#by Rida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkin Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "# -----------------------------------------------------------\n",
    "# Requirement V.4: Rank all businesses based on check-in counts\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "# Step 1: Split and explode check-in dates to individual timestamps\n",
    "checkin_exploded = checkin.withColumn(\"checkin_date\", explode(split(col(\"checkin_dates\"), \",\")))\n",
    "\n",
    "# Step 2: Count total check-ins for each business\n",
    "business_checkin_counts = checkin_exploded.groupBy(\"business_id\").agg(count(\"*\").alias(\"total_checkins\"))\n",
    "\n",
    "# Step 3: Join with business dataset to get business names and cities (optional)\n",
    "ranked_businesses = business_checkin_counts.join(\n",
    "    business.select(\"business_id\", \"name\", \"city\"), \n",
    "    \"business_id\", \n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "# Step 4: Order by check-in count descending to get ranking\n",
    "ranked_businesses.orderBy(col(\"total_checkins\").desc()).show(20, False)\n",
    "\n",
    "\n",
    "#by Rida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprehensive Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Comprehensive Analysis: Top 5 merchants per city \n",
    "# Based on review count, average rating, check-in count\n",
    "# -----------------------------------------------\n",
    "\n",
    "from pyspark.sql.functions import col, count, avg, split, explode, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# --- Review count and average rating per business ---\n",
    "review_stats = review.groupBy(\"rev_business_id\").agg(\n",
    "    count(\"*\").alias(\"review_count\"),\n",
    "    avg(\"rev_stars\").alias(\"average_rating\")\n",
    ")\n",
    "\n",
    "# --- Check-in count per business ---\n",
    "checkin_exploded = checkin.withColumn(\"checkin_date\", explode(split(col(\"checkin_dates\"), \",\")))\n",
    "checkin_stats = checkin_exploded.groupBy(\"business_id\").agg(count(\"*\").alias(\"checkin_count\"))\n",
    "\n",
    "# --- Join all stats with business info ---\n",
    "biz_info = business.select(\"business_id\", \"name\", \"city\")\n",
    "stats = biz_info \\\n",
    "    .join(review_stats, biz_info.business_id == review_stats.rev_business_id, \"left\") \\\n",
    "    .join(checkin_stats, \"business_id\", \"left\") \\\n",
    "    .fillna(0, subset=[\"review_count\", \"average_rating\", \"checkin_count\"])\n",
    "\n",
    "# --- Rank top 5 merchants in each city ---\n",
    "window = Window.partitionBy(\"city\").orderBy(\n",
    "    col(\"review_count\").desc(),\n",
    "    col(\"average_rating\").desc(),\n",
    "    col(\"checkin_count\").desc()\n",
    ")\n",
    "top5 = stats.withColumn(\"rank\", row_number().over(window)).filter(col(\"rank\") <= 5)\n",
    "\n",
    "# --- Show final result ---\n",
    "top5.select(\"city\", \"name\", \"review_count\", \"average_rating\", \"checkin_count\", \"rank\") \\\n",
    "    .orderBy(\"city\", \"rank\").show(100, False)\n",
    "\n",
    "\n",
    "#by Rida "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "# 2. Count useful, funny, cool reviews\n",
    "review.agg(sum(\"rev_useful\").alias(\"useful_votes\"), sum(\"rev_funny\").alias(\"funny_votes\"), sum(\"rev_cool\").alias(\"cool_votes\")).show(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "# 3. Rank users by total number of reviews each year\n",
    "review.withColumn(\"review_year\", year(to_date(col(\"rev_date\"), \"yyyy-MM-dd\"))).groupBy(\"review_year\", \"rev_user_id\").agg(count(\"*\").alias(\"review_count\")).orderBy(desc(\"review_count\")).show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
