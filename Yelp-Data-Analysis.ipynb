{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Business Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%pyspark\n",
    "\n",
    "# 4. Extract Top 20 most common words from all reviews (stopwords removed)\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from pyspark.sql.functions import explode, col, lower\n",
    "\n",
    "# Tokenize review texts\n",
    "tokenized = Tokenizer(inputCol=\"rev_text\", outputCol=\"words\").transform(review)\n",
    " \n",
    "# Remove stopwords\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "filtered = remover.transform(tokenized)\n",
    "\n",
    "# Explode words, convert to lowercase, group, count, order, and show top 20\n",
    "filtered.select(explode(col(\"filtered_words\")).alias(\"word\")) \\\n",
    "    .select(lower(col(\"word\")).alias(\"word\")) \\\n",
    "    .groupBy(\"word\").count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show(20, truncate=False)\n",
    "\n",
    "\n",
    "#by RIDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "# 5. Extract Top 10 words from positive reviews (rating > 3), stopwords removed\n",
    "positive_reviews = review.filter(col(\"rev_stars\") > 3)\n",
    "\n",
    "# Tokenize and remove stopwords\n",
    "tokenized_pos = Tokenizer(inputCol=\"rev_text\", outputCol=\"words\").transform(positive_reviews)\n",
    "filtered_pos = remover.transform(tokenized_pos)\n",
    "\n",
    "# Explode, lowercase, count, and show top 10\n",
    "filtered_pos.select(explode(col(\"filtered_words\")).alias(\"word\")) \\\n",
    "    .select(lower(col(\"word\")).alias(\"word\")) \\\n",
    "    .groupBy(\"word\").count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show(10, truncate=False)\n",
    "\n",
    "\n",
    "#by RIDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "# 6. Extract Top 10 words from negative reviews (rating â‰¤ 3), stopwords removed\n",
    "negative_reviews = review.filter(col(\"rev_stars\") <= 3)\n",
    "\n",
    "# Tokenize and remove stopwords\n",
    "tokenized_neg = Tokenizer(inputCol=\"rev_text\", outputCol=\"words\").transform(negative_reviews)\n",
    "filtered_neg = remover.transform(tokenized_neg)\n",
    "\n",
    "# Explode, lowercase, count, and show top 10\n",
    "filtered_neg.select(explode(col(\"filtered_words\")).alias(\"word\")) \\\n",
    "    .select(lower(col(\"word\")).alias(\"word\")) \\\n",
    "    .groupBy(\"word\").count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show(10, truncate=False)\n",
    "\n",
    "\n",
    "#by RIDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rating Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark \n",
    "\n",
    "from pyspark.sql.functions import split, explode, to_timestamp, year, hour, count, trim, col\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 1. Count the number of check-ins per year\n",
    "# ---------------------------------------------\n",
    "# Step 1: Split check-in dates and explode into individual timestamps\n",
    "checkin_exploded = checkin.withColumn(\"checkin_date\", explode(split(col(\"checkin_dates\"), \",\")))\n",
    "\n",
    "# Step 2: Extract year and count check-ins per year\n",
    "checkin_exploded.withColumn(\"checkin_date_ts\", to_timestamp(trim(col(\"checkin_date\")), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "    .withColumn(\"year\", year(col(\"checkin_date_ts\"))) \\\n",
    "    .groupBy(\"year\").agg(count(\"*\").alias(\"checkin_count\")) \\\n",
    "    .orderBy(\"year\").show(10, False)\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 2. Count the number of check-ins per hour within a 24-hour period\n",
    "# ---------------------------------------------\n",
    "# Step 1 & 2 reused: already exploded and timestamped\n",
    "# Step 3: Extract hour and count\n",
    "checkin_exploded.withColumn(\"checkin_date_ts\", to_timestamp(trim(col(\"checkin_date\")), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "    .withColumn(\"hour\", hour(col(\"checkin_date_ts\"))) \\\n",
    "    .groupBy(\"hour\").agg(count(\"*\").alias(\"checkin_count\")) \\\n",
    "    .orderBy(\"hour\").show(24, False)\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 3. Identify the most popular city for check-ins\n",
    "# ---------------------------------------------\n",
    "# Join check-in data with business to get city names, then count check-ins per city\n",
    "checkin_exploded.join(business, checkin_exploded.business_id == business.business_id, \"inner\") \\\n",
    "    .groupBy(\"city\").agg(count(\"*\").alias(\"total_checkins\")) \\\n",
    "    .orderBy(col(\"total_checkins\").desc()).show(10, False)\n",
    "\n",
    "\n",
    "\n",
    "#by Rida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkin Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "# -----------------------------------------------------------\n",
    "# Requirement V.4: Rank all businesses based on check-in counts\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "# Step 1: Split and explode check-in dates to individual timestamps\n",
    "checkin_exploded = checkin.withColumn(\"checkin_date\", explode(split(col(\"checkin_dates\"), \",\")))\n",
    "\n",
    "# Step 2: Count total check-ins for each business\n",
    "business_checkin_counts = checkin_exploded.groupBy(\"business_id\").agg(count(\"*\").alias(\"total_checkins\"))\n",
    "\n",
    "# Step 3: Join with business dataset to get business names and cities (optional)\n",
    "ranked_businesses = business_checkin_counts.join(\n",
    "    business.select(\"business_id\", \"name\", \"city\"), \n",
    "    \"business_id\", \n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "# Step 4: Order by check-in count descending to get ranking\n",
    "ranked_businesses.orderBy(col(\"total_checkins\").desc()).show(20, False)\n",
    "\n",
    "\n",
    "#by Rida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprehensive Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Comprehensive Analysis: Top 5 merchants per city \n",
    "# Based on review count, average rating, check-in count\n",
    "# -----------------------------------------------\n",
    "\n",
    "from pyspark.sql.functions import col, count, avg, split, explode, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# --- Review count and average rating per business ---\n",
    "review_stats = review.groupBy(\"rev_business_id\").agg(\n",
    "    count(\"*\").alias(\"review_count\"),\n",
    "    avg(\"rev_stars\").alias(\"average_rating\")\n",
    ")\n",
    "\n",
    "# --- Check-in count per business ---\n",
    "checkin_exploded = checkin.withColumn(\"checkin_date\", explode(split(col(\"checkin_dates\"), \",\")))\n",
    "checkin_stats = checkin_exploded.groupBy(\"business_id\").agg(count(\"*\").alias(\"checkin_count\"))\n",
    "\n",
    "# --- Join all stats with business info ---\n",
    "biz_info = business.select(\"business_id\", \"name\", \"city\")\n",
    "stats = biz_info \\\n",
    "    .join(review_stats, biz_info.business_id == review_stats.rev_business_id, \"left\") \\\n",
    "    .join(checkin_stats, \"business_id\", \"left\") \\\n",
    "    .fillna(0, subset=[\"review_count\", \"average_rating\", \"checkin_count\"])\n",
    "\n",
    "# --- Rank top 5 merchants in each city ---\n",
    "window = Window.partitionBy(\"city\").orderBy(\n",
    "    col(\"review_count\").desc(),\n",
    "    col(\"average_rating\").desc(),\n",
    "    col(\"checkin_count\").desc()\n",
    ")\n",
    "top5 = stats.withColumn(\"rank\", row_number().over(window)).filter(col(\"rank\") <= 5)\n",
    "\n",
    "# --- Show final result ---\n",
    "top5.select(\"city\", \"name\", \"review_count\", \"average_rating\", \"checkin_count\", \"rank\") \\\n",
    "    .orderBy(\"city\", \"rank\").show(100, False)\n",
    "\n",
    "\n",
    "#by Rida "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "# 2. Count useful, funny, cool reviews\n",
    "review.agg(sum(\"rev_useful\").alias(\"useful_votes\"), sum(\"rev_funny\").alias(\"funny_votes\"), sum(\"rev_cool\").alias(\"cool_votes\")).show(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "# 3. Rank users by total number of reviews each year\n",
    "review.withColumn(\"review_year\", year(to_date(col(\"rev_date\"), \"yyyy-MM-dd\"))).groupBy(\"review_year\", \"rev_user_id\").agg(count(\"*\").alias(\"review_count\")).orderBy(desc(\"review_count\")).show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
