{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%sh\n",
        "pip install graphframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.sql.functions import *\n",
        "import os\n",
        "\n",
        "# Initialize Spark with GraphFrames and OOM protection\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"UnifiedRecSys\") \\\n",
        "    .config(\"spark.sql.warehouse.dir\", \"/user/hive/warehouse\") \\\n",
        "    .config(\"spark.executor.memory\", \"4g\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"50\") \\\n",
        "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
        "    .config(\"spark.jars.packages\", \"graphframes:graphframes:0.8.3-spark3.1-s_2.12\") \\\n",
        "    .enableHiveSupport() \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Import GraphFrame AFTER Spark session is initialized\n",
        "from graphframes import GraphFrame\n",
        "\n",
        "# Configure your custom ChatGPT client\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    base_url='https://xiaoai.plus/v1',\n",
        "    api_key='sk-WvIc4NVMTcUwuqa5xVrHG0VG3V2m2xeAK9Umx0NlDD4ZLPFL'\n",
        ")\n",
        "\n",
        "\n",
        "# By RIDA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.storagelevel import StorageLevel  # Critical import\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"BusinessRecSys\") \\\n",
        "    .config(\"spark.driver.memory\", \"8g\") \\\n",
        "    .config(\"spark.executor.memory\", \"8g\") \\\n",
        "    .enableHiveSupport() \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Load data with proper persistence\n",
        "business = spark.table(\"business\").persist(StorageLevel.DISK_ONLY)\n",
        "review = spark.table(\"review\").persist(StorageLevel.DISK_ONLY) \n",
        "users = spark.table(\"users\").persist(StorageLevel.MEMORY_AND_DISK)\n",
        "\n",
        "print(f\"\"\"\n",
        "Loaded Data:\n",
        "- Businesses: {business.count():,}\n",
        "- Reviews: {review.count():,}\n",
        "- Users: {users.count():,}\n",
        "\"\"\")\n",
        "#By Rida"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "# Show a few records to check if everything is fine\n",
        "business.show(5)\n",
        "checkin.show(5)\n",
        "review.show(5)\n",
        "\n",
        "\n",
        "#By Rida"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "print(f\"Business Data Count: {business.count()}\")\n",
        "print(f\"Check-in Data Count: {checkin.count()}\")\n",
        "print(f\"Review Data Count: {review.count()}\")\n",
        "\n",
        "#By Rida"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "# 1. Start fresh - drop existing indexed columns\n",
        "review = review.select(\"user_id\", \"business_id\", \"rating\")\n",
        "\n",
        "# 2. Create new indexers\n",
        "user_indexer = StringIndexer(inputCol=\"user_id\", outputCol=\"user_index\")\n",
        "business_indexer = StringIndexer(inputCol=\"business_id\", outputCol=\"business_index\")\n",
        "\n",
        "# 3. Fit models (using distinct values)\n",
        "user_index_model = user_indexer.fit(review.select(\"user_id\").distinct())\n",
        "business_index_model = business_indexer.fit(review.select(\"business_id\").distinct())\n",
        "\n",
        "# 4. Apply transformations\n",
        "review = user_index_model.transform(review)\n",
        "review = business_index_model.transform(review)\n",
        "\n",
        "# 5. Keep only essential columns\n",
        "review = review.select(\"user_index\", \"business_index\", \"rating\").persist()\n",
        "\n",
        "# 6. Verify\n",
        "print(\"Final Schema After Fix:\")\n",
        "review.printSchema()\n",
        "\n",
        "\n",
        "\n",
        "#By Rida"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "# Increase to 20%\n",
        "review = review.sample(0.05).persist()\n",
        "review.count() \n",
        "\n",
        "#By Rida"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "from pyspark.ml.recommendation import ALS\n",
        "\n",
        "# Train ALS with memory optimization\n",
        "als = ALS(\n",
        "    rank=8, \n",
        "    maxIter=3,  \n",
        "    regParam=0.1,\n",
        "    userCol=\"user_index\",\n",
        "    itemCol=\"business_index\",\n",
        "    ratingCol=\"rating\",\n",
        "    coldStartStrategy=\"drop\"\n",
        ")\n",
        "\n",
        "als_model = als.fit(review)\n",
        "user_factors = als_model.userFactors.cache()\n",
        "\n",
        "#By khadija\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "\n",
        "%pyspark\n",
        "from pyspark.ml.linalg import Vectors, VectorUDT\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
        "\n",
        "# ------------------------------------\n",
        "# 1. Convert & Optimize User Factors\n",
        "# ------------------------------------\n",
        "list_to_vector_udf = F.udf(lambda lst: Vectors.dense(lst), VectorUDT())\n",
        "\n",
        "user_factors = als_model.userFactors.select(\n",
        "    F.col(\"id\").cast(\"integer\"),\n",
        "    list_to_vector_udf(F.col(\"features\")).alias(\"features\")\n",
        ").repartition(100, \"id\").alias(\"u1\")  # Remove the .filter() with last_active_date\n",
        "\n",
        "user_factors.cache().count()\n",
        "\n",
        "# ------------------------------------\n",
        "# 2. Optimized LSH Configuration\n",
        "# ------------------------------------\n",
        "brp = BucketedRandomProjectionLSH(\n",
        "    inputCol=\"features\",\n",
        "    outputCol=\"hashes\",\n",
        "    bucketLength=4.0,\n",
        "    numHashTables=2\n",
        ")\n",
        "\n",
        "lsh_model = brp.fit(user_factors)\n",
        "\n",
        "# ------------------------------------\n",
        "# 3. Limited Similarity Search\n",
        "# ------------------------------------\n",
        "similar_users = lsh_model.approxSimilarityJoin(\n",
        "    user_factors, \n",
        "    user_factors, \n",
        "    threshold=2.0,\n",
        "    distCol=\"distance\"\n",
        ").filter(\"datasetA.id < datasetB.id\").limit(500000)\n",
        "\n",
        "# ------------------------------------\n",
        "# 4. Similarity Calculation\n",
        "# ------------------------------------\n",
        "friend_recs = similar_users.select(\n",
        "    F.col(\"datasetA.id\").alias(\"user1\"),\n",
        "    F.col(\"datasetB.id\").alias(\"user2\"),\n",
        "    (1 - F.col(\"distance\")).alias(\"similarity\")\n",
        ").filter(F.col(\"similarity\") > 0.2)\n",
        "\n",
        "# ------------------------------------\n",
        "# 5. Top-N Recommendations\n",
        "# ------------------------------------\n",
        "window = Window.partitionBy(\"user1\").orderBy(F.desc(\"similarity\"))\n",
        "\n",
        "top_similar_users = (friend_recs\n",
        "    .withColumn(\"rank\", F.row_number().over(window))\n",
        "    .filter(F.col(\"rank\") <= 5)\n",
        "    .select(\"user1\", \"user2\", \"similarity\")\n",
        ")\n",
        "\n",
        "# ------------------------------------\n",
        "# 6. Execute and Monitor\n",
        "# ------------------------------------\n",
        "print(f\"Processing {user_factors.count()} users\")\n",
        "top_similar_users.show(20, truncate=False)\n",
        "\n",
        "#By khadija"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# 1. Generate Similar Users (LSH)\n",
        "# -----------------------------------------------------\n",
        "# Assuming you've already created user_factors DataFrame\n",
        "brp = BucketedRandomProjectionLSH(\n",
        "    inputCol=\"features\",\n",
        "    outputCol=\"hashes\",\n",
        "    bucketLength=2.0,\n",
        "    numHashTables=3\n",
        ")\n",
        "lsh_model = brp.fit(user_factors)\n",
        "\n",
        "# Calculate similar users (THIS CREATES similar_users)\n",
        "similar_users = lsh_model.approxSimilarityJoin(\n",
        "    user_factors, \n",
        "    user_factors, \n",
        "    threshold=1.5, \n",
        "    distCol=\"distance\"\n",
        ")\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# 2. Create User Pairs\n",
        "# -----------------------------------------------------\n",
        "user_pairs = similar_users.select(\n",
        "    F.col(\"datasetA.id\").alias(\"user1\"),\n",
        "    F.col(\"datasetB.id\").alias(\"user2\"),\n",
        "    (1 - F.col(\"distance\")).alias(\"similarity\")\n",
        ").filter(F.col(\"similarity\") > 0.2)\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# 3. Get Top Recommendations\n",
        "# -----------------------------------------------------\n",
        "window = Window.partitionBy(\"user1\").orderBy(F.desc(\"similarity\"))\n",
        "\n",
        "top_friend_recs = user_pairs \\\n",
        "    .withColumn(\"rank\", F.row_number().over(window)) \\\n",
        "    .filter(F.col(\"rank\") <= 5) \\\n",
        "    .drop(\"rank\")\n",
        "\n",
        "top_friend_recs.show(10)\n",
        "#By khadija"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "\n",
        "%pyspark\n",
        "# Get users and their reviewed businesses\n",
        "user_reviews = review.select(\n",
        "    col(\"user_index\").alias(\"user\"),\n",
        "    col(\"business_index\").alias(\"business\")\n",
        ")\n",
        "\n",
        "# Find common businesses between recommended pairs\n",
        "common_businesses = top_friend_recs.join(\n",
        "    user_reviews.alias(\"u1\"), \n",
        "    col(\"user1\") == col(\"u1.user\")\n",
        ").join(\n",
        "    user_reviews.alias(\"u2\"), \n",
        "    (col(\"user2\") == col(\"u2.user\")) & \n",
        "    (col(\"u1.business\") == col(\"u2.business\"))\n",
        ").groupBy(\"user1\", \"user2\").agg(\n",
        "    count(\"*\").alias(\"shared_businesses\")\n",
        ")\n",
        "\n",
        "\n",
        "# Show users with the most shared interests\n",
        "common_businesses.orderBy(col(\"shared_businesses\").desc()).show()\n",
        "\n",
        "#BY AYA \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%sh\n",
        "pip install pyarrow==14.0.0  # Install specific version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import StringType\n",
        "import os\n",
        "\n",
        "def generate_explanation(user1_id, user2_id):\n",
        "    # Initialize OpenAI client inside UDF (fresh for each task)\n",
        "    from openai import OpenAI\n",
        "    client = OpenAI(\n",
        "        base_url=\"https://xiaoai.plus/v1\",\n",
        "        api_key=os.environ[\"OPENAI_API_KEY\"]  # Store key in Zeppelin environment\n",
        "    )\n",
        "    \n",
        "    # Fetch user data safely\n",
        "    user1_reviews = review.where(F.col(\"user_index\") == user1_id).limit(3).collect()\n",
        "    user2_reviews = review.where(F.col(\"user_index\") == user2_id).limit(3).collect()\n",
        "    \n",
        "    # Build prompt\n",
        "    prompt = f\"Explain why user {user1_id} and {user2_id} should connect based on shared interests.\"\n",
        "    \n",
        "    # API call with error handling\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            timeout=10  # Prevent hanging\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        return f\"Could not generate explanation: {str(e)}\"\n",
        "\n",
        "# Register UDF\n",
        "explanation_udf = F.udf(generate_explanation, StringType())\n",
        "\n",
        "\n",
        "#by AYA "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%sh\n",
        "# Install networkx and matplotlib in the Spark/Python environment\n",
        "pip install networkx matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# 1. Generate recommendations (if not already done)\n",
        "window = Window.partitionBy(\"user1\").orderBy(F.desc(\"similarity\"))\n",
        "top_friend_recs = user_pairs.withColumn(\"rank\", F.row_number().over(window)) \\\n",
        "    .filter(F.col(\"rank\") <= 5) \\\n",
        "    .drop(\"rank\")\n",
        "\n",
        "# 2. Create final_recs\n",
        "final_recs = top_friend_recs  # Or add any additional transformations\n",
        "final_recs.persist().count()  # Force materialization \n",
        "#BY Mohamed "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert Spark DataFrame to Pandas\n",
        "pandas_recs = final_recs.toPandas()\n",
        "\n",
        "# Create graph\n",
        "G = nx.Graph()\n",
        "for _, row in pandas_recs.iterrows():\n",
        "    G.add_edge(row[\"user1\"], row[\"user2\"], weight=row[\"similarity\"])\n",
        "\n",
        "# Draw\n",
        "plt.figure(figsize=(12, 8))\n",
        "nx.draw(G, with_labels=True, node_color='skyblue', edge_color='gray', width=1.5)\n",
        "plt.title(\"Friend Recommendations Based on ALS Similarity\")\n",
        "plt.show() \n",
        "#BY Mohamed"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    },
    "name": "Friend-Recomendation"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
